




//parsing

1) recuperer la ligne de commande avec readline
2) reussir a separer tous les token en fonction de tous les cas possible (quoate,...)
    a) bien différentcier les elt de citations qd ils sont present en simple char
        - "" preserve valeur littérale de chaque caractère. Ne peut pas conteir de
                - `
                - $ : Les caractères d'entrée de la chaîne entre guillemets qui sont également compris
                entre "$(" et le ")" correspondant ne sont pas affectés par les guillemets, mais définissent
                plutôt la commande dont la sortie remplace le "$(...)" 
                - \
                - @
        - '' preserve valeur littérale de chaque caractère. Ne peut pas contenir de simple quoat
        - Les opérateurs de redirection "<<" et "<<-" permettent tous deux de rediriger
        les lignes contenues dans un fichier d'entrée de l'interpréteur de commandes, appelé 
        "here-document", vers l'entrée d'une commande.
        - \ correspond à une continuation de ligne. Doit être enlever des token si elle n'est pas cité
        - $ introduit l'expansion des paramètres
3) voir env -i
4) si qu'un seul mot et ce n'est pas une commande, error
// Token recognisation

//Builting (fonction a recoder)

- cd 
- pwd
- export
- unset
- env
- exit 





question :

- comment identifier si il y a des quotes etant donné que le shell de base le gère déjà





TOKENIZATION

/* étapes */

//1 récuperer ligne de commande (fichier input)
//2 analyse lexicale : découper la ligne de commande en token grace aux séparateurs
    //espaces, tabulations
    //retour à la ligne
    //guillements
//3 Analyse syntaxique : identifier la structure syntaxique de la ligne de commande (le type)
//4 Analyse sémantique : interpréter les tokens pour leur donner un sens (ex identifier les cmd, les variables, les options,...)
//5 execution


Analyse lexicale :
//1 chaîne de caractères de la ligne de commande lue et stockée
//2 elimination des caractères blancs (espace, tab, retour à la ligne)
//3 détéction des guillemets
//4 division en tokens
//5 retour des tokens


analyse syntaxique :
/* Vérifier que la séquence d'elt lexicaux créée pdt l'analyse lexical est cohérente avec la grammaire du langage (respecte règles syntaxiques)*/

//1 création de l'arbre syntaxique: séquence d'éléments lexicaux est analysée pour créer un arbre s, qui représente la structure syntaxique de l'expression
//2 Validation de la grammaire: arbre validé pour assurer qu'il respect regle de grammaire. ex: vérifie que les opérateurs sont utilisés correctement, instruction correctement imbriqués
//3 detection erreurs de syntaxe
//4 gestion des priorités: inclure étape pour gerer prio des opérateurs
//5 transformation de l'arbre


analyse semantique :
/*but : vérifier que les elt lexicaux et les expressions syntaxiques crées pdt les étapes précédentes
sont cohérentes sémantiquement parlant: qu'ils ont un sens et une interprétation valides*/

//1 verificaton des types. ex: les opérateurs sont appliqués à des types compatibles, les fonctions sont appelées avec les bons arguments, les var sont déclarées et initialisées correctement
//2 Résolution des noms: les noms des fonctions, variables, et autres symboles sont résolus: ex verif que fonctions appelées avc le bon nom
    //que les var sont déclarées avant leur utilisation, symboles def dans le bon espace de nom, etc
//3 Analyse de flux de contrôle: on verifie que instructions de boucle ont des contions d'arrêt valide, instructions d'exception correctement gérée, etc
//4 génération de code intermediaire: une fois que elt lexicaux et expression ont été verif sémantiquement, un code interm peut être généré pour faciliter l'opti et la generation du code final
